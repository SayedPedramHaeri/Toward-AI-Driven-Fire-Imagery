{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09720fc2-5d29-438e-8799-c06c21033e1f",
   "metadata": {},
   "source": [
    "# Classification — ResNet-50\n",
    "**Author:** Sayed Pedram Haeri Boroujeni  \n",
    "**Position:** PhD Student, Clemson University  \n",
    "**Affiliation:** Department of Computer Science  \n",
    "**Email:** shaerib@g.clemson.edu  \n",
    "**Date Created:** October 10, 2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f649a-a69d-43e3-9aa0-9537063b5a8d",
   "metadata": {},
   "source": [
    "##### 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501c692-3069-455b-b3b8-e9dff9873a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, random, time, math, csv, shutil, glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f37870-4b7b-467d-85c0-776bc6761585",
   "metadata": {},
   "source": [
    "##### 2. Checking GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f74139-a333-411b-bc84-d741371b4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae7d4a-68a3-4173-8746-e2e63bb0970b",
   "metadata": {},
   "source": [
    "##### 3. Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd5c8b-0d43-41a1-87fb-93a997dde6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e7140-d5d8-4527-9865-6ec577569a7e",
   "metadata": {},
   "source": [
    "##### 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abb404-0e46-43a6-b188-59d86148e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"ADD YOUR PATH HERE\"  \n",
    "data_dir = \"C:/Users/shaerib/OneDrive - Clemson University/Desktop/Dataset/Fire\"  \n",
    "\n",
    "\n",
    "use_kfold = True      # Set True for K-Fold, False for fixed split\n",
    "num_folds = 5          # K value if K-Fold is used\n",
    "num_epochs = 30\n",
    "batch_size = 16\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f7630-add8-452f-866f-12c264786ed7",
   "metadata": {},
   "source": [
    "##### 5. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298668ea-3ca2-46af-80bd-110ec7c7c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(root_dir, img_size=224):\n",
    "    \"\"\"\n",
    "    Prepare dataset for training:\n",
    "    1. Detect class subfolders (e.g., Fire, no_Fire).\n",
    "    2. Split each class (60/20/20) into train/val/test while keeping subfolders.\n",
    "    3. Resize and rename images.\n",
    "    4. Report per-class counts.\n",
    "    \"\"\"\n",
    "    from collections import defaultdict\n",
    "\n",
    "    root_dir = Path(root_dir)\n",
    "    train_dir, val_dir, test_dir = [root_dir / x for x in [\"train\", \"val\", \"test\"]]\n",
    "    exts = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\", \"*.webp\"]\n",
    "\n",
    "    def all_images_in(folder):\n",
    "        imgs = []\n",
    "        for ext in exts:\n",
    "            imgs.extend(Path(folder).rglob(ext))\n",
    "        return imgs\n",
    "\n",
    "    # Detect original class folders\n",
    "    class_folders = [\n",
    "        f for f in root_dir.iterdir()\n",
    "        if f.is_dir() and f.name.lower() not in [\"train\", \"val\", \"test\"]\n",
    "    ]\n",
    "    if not class_folders:\n",
    "        raise RuntimeError(\"No class subfolders found (expected 'Fire', 'no_Fire').\")\n",
    "\n",
    "    # Create split dirs and subdirs\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for cls in class_folders:\n",
    "            (root_dir / split / cls.name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    counts = defaultdict(lambda: {\"train\": 0, \"val\": 0, \"test\": 0})\n",
    "\n",
    "    print(\"⚙️ Splitting dataset by class into train (60%), val (20%), test (20%)...\")\n",
    "    for cls_folder in class_folders:\n",
    "        cls_name = cls_folder.name\n",
    "        images = []\n",
    "        for ext in exts:\n",
    "            images.extend(cls_folder.rglob(ext))\n",
    "        images = [p for p in images if p.is_file()]\n",
    "        if not images:\n",
    "            print(f\"⚠️ No images found in {cls_name}\")\n",
    "            continue\n",
    "\n",
    "        # Split 60/20/20 for this class\n",
    "        train_files, temp_files = train_test_split(images, test_size=0.4, random_state=42)\n",
    "        val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)\n",
    "        splits = {\"train\": train_files, \"val\": val_files, \"test\": test_files}\n",
    "\n",
    "        for split, files in splits.items():\n",
    "            out_dir = root_dir / split / cls_name\n",
    "            for i, file_path in enumerate(tqdm(files, desc=f\"{cls_name} → {split}\", ncols=90)):\n",
    "                try:\n",
    "                    img = Image.open(file_path).convert(\"RGB\")\n",
    "                    img = img.resize((img_size, img_size))\n",
    "                    new_name = f\"{i+1}.jpg\"\n",
    "                    out_path = out_dir / new_name\n",
    "                    img.save(out_path, \"JPEG\", quality=95)\n",
    "                except Exception as e:\n",
    "                    print(f\"❌ Error processing {file_path}: {e}\")\n",
    "            counts[cls_name][split] += len(files)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n✅ Dataset split completed successfully with class subfolders:\")\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        total = sum(counts[c][split] for c in counts)\n",
    "        print(f\"  {split.capitalize()}: {total} images\")\n",
    "        for cls in counts:\n",
    "            print(f\"    {cls}: {counts[cls][split]} images\")\n",
    "\n",
    "    return counts\n",
    "\n",
    "#For Testing\n",
    "#prepare_dataset(\"C:/Users/shaerib/OneDrive - Clemson University/Desktop/Dataset/Flame1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48e228-563c-4912-88f8-2f5f55826f46",
   "metadata": {},
   "source": [
    "##### 6. Data Loading & Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaca8a3-0004-4b79-93f3-2e3e589d4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization values for pretrained ResNet\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"train\"), transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"val\"), transform=val_test_transform)\n",
    "test_dataset = datasets.ImageFolder(root=os.path.join(data_dir, \"test\"), transform=val_test_transform)\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} images\")\n",
    "print(f\"Validation: {len(val_dataset)} images\")\n",
    "print(f\"Test: {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06284569-ddb6-47c3-b7d8-ffdbd32a3e57",
   "metadata": {},
   "source": [
    "##### 7. ResNet-50 Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37f6fe-b0df-4cee-8803-d22896fb554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_classes = len(train_dataset.classes)\n",
    "\n",
    "# Initialize pretrained ResNet-50\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(momentum, 0.999))\n",
    "\n",
    "print(\"✅ ResNet-50 initialized successfully\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Classes: {train_dataset.classes}\")\n",
    "print(f\"Learning rate: {learning_rate}, Momentum (β₁): {momentum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4957df-53d4-4b03-a34c-40468c585ccb",
   "metadata": {},
   "source": [
    "##### 8. ResNet-50 Training & Validation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849a0480-db82-4ff5-a61b-db3d2dc45662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### 8. ResNet-50 Training & Optional K-Fold Cross-Validation (stores histories) #####\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader,\n",
    "                num_epochs, device='cuda', fold=None):\n",
    "    \"\"\"Training loop with validation, best model saving, and metric recording.\"\"\"\n",
    "    train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss, running_corrects = 0.0, 0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\", ncols=90):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc.item())\n",
    "\n",
    "        # ---- Validation ----\n",
    "        model.eval()\n",
    "        val_loss, val_corrects = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc.item())\n",
    "\n",
    "        print(f\"[{epoch:02d}/{num_epochs}] \"\n",
    "              f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            model_name = f\"best_resnet50_fire_fold{fold}.pth\" if fold else \"best_resnet50_fire.pth\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "\n",
    "    print(f\"\\n✅ Training completed. Best Val Accuracy: {best_acc:.4f}\")\n",
    "    return train_losses, val_losses, train_accs, val_accs, best_acc\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  K-Fold Cross-Validation or Fixed Split\n",
    "# ============================================================\n",
    "\n",
    "if use_kfold:\n",
    "    print(f\"🔁 Running {num_folds}-Fold Cross-Validation …\\n\")\n",
    "\n",
    "    full_dataset = datasets.ImageFolder(os.path.join(data_dir, \"train\"), transform=train_transform)\n",
    "    all_labels = [label for _, label in full_dataset.samples]\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "    fold_histories = {}   # 📊 store loss/accuracy for each fold\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(all_labels)), all_labels)):\n",
    "        print(f\"\\n===== Fold {fold+1}/{num_folds} =====\")\n",
    "\n",
    "        train_subset = Subset(full_dataset, train_idx)\n",
    "        val_subset   = Subset(full_dataset, val_idx)\n",
    "\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "        val_loader   = DataLoader(val_subset,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "        # Re-initialize model each fold\n",
    "        model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "        model.fc = nn.Linear(model.fc.in_features, len(full_dataset.classes))\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(momentum, 0.999))\n",
    "\n",
    "        tr_loss, vl_loss, tr_acc, vl_acc, best_acc = train_model(\n",
    "            model, criterion, optimizer, train_loader, val_loader,\n",
    "            num_epochs=num_epochs, device=device, fold=fold+1\n",
    "        )\n",
    "\n",
    "        fold_results.append(best_acc.item())\n",
    "\n",
    "        # Store curves for this fold\n",
    "        fold_histories[fold+1] = {\n",
    "            \"train_losses\": tr_loss,\n",
    "            \"val_losses\": vl_loss,\n",
    "            \"train_accs\": tr_acc,\n",
    "            \"val_accs\": vl_acc,\n",
    "            \"best_acc\": best_acc.item()\n",
    "        }\n",
    "\n",
    "    print(\"\\n✅ Cross-validation complete.\")\n",
    "    print(f\"Average Val Accuracy: {np.mean(fold_results):.4f} ± {np.std(fold_results):.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"➡️ Using fixed train/val/test split.\\n\")\n",
    "\n",
    "    tr_loss, vl_loss, tr_acc, vl_acc, best_acc = train_model(\n",
    "        model, criterion, optimizer, train_loader, val_loader,\n",
    "        num_epochs=num_epochs, device=device\n",
    "    )\n",
    "\n",
    "    # ✅ Store results for plotting later\n",
    "    fold_histories = {\n",
    "        0: {\n",
    "            \"train_losses\": tr_loss,\n",
    "            \"val_losses\": vl_loss,\n",
    "            \"train_accs\": tr_acc,\n",
    "            \"val_accs\": vl_acc,\n",
    "            \"best_acc\": best_acc.item()\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"✅ Fixed-split training complete. Best Val Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9c962-08f1-4cdf-9b2d-9f79a4f58d50",
   "metadata": {},
   "source": [
    "##### 9. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c6e22-4a66-425c-952e-db4703ffa1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves_kfold(histories, num_folds, title=\"ResNet-50 K-Fold Training Curves\"):\n",
    "    \"\"\"Plot all folds together + average trend.\"\"\"\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # --- Loss Curves ---\n",
    "    plt.subplot(1,2,1)\n",
    "    for fold, h in histories.items():\n",
    "        plt.plot(h['train_losses'], alpha=0.5, label=f\"Train Fold {fold}\")\n",
    "        plt.plot(h['val_losses'], linestyle='--', alpha=0.5, label=f\"Val Fold {fold}\")\n",
    "    # average curves\n",
    "    max_epochs = min(len(h['train_losses']) for h in histories.values())\n",
    "    avg_train_loss = np.mean([h['train_losses'][:max_epochs] for h in histories.values()], axis=0)\n",
    "    avg_val_loss = np.mean([h['val_losses'][:max_epochs] for h in histories.values()], axis=0)\n",
    "    plt.plot(avg_train_loss, color='blue', linewidth=2.5, label=\"Avg Train Loss\")\n",
    "    plt.plot(avg_val_loss, color='red', linewidth=2.5, label=\"Avg Val Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Across Folds\")\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # --- Accuracy Curves ---\n",
    "    plt.subplot(1,2,2)\n",
    "    for fold, h in histories.items():\n",
    "        plt.plot(h['train_accs'], alpha=0.5, label=f\"Train Fold {fold}\")\n",
    "        plt.plot(h['val_accs'], linestyle='--', alpha=0.5, label=f\"Val Fold {fold}\")\n",
    "    avg_train_acc = np.mean([h['train_accs'][:max_epochs] for h in histories.values()], axis=0)\n",
    "    avg_val_acc = np.mean([h['val_accs'][:max_epochs] for h in histories.values()], axis=0)\n",
    "    plt.plot(avg_train_acc, color='blue', linewidth=2.5, label=\"Avg Train Acc\")\n",
    "    plt.plot(avg_val_acc, color='red', linewidth=2.5, label=\"Avg Val Acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Across Folds\")\n",
    "    plt.legend(fontsize=8)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_training_curves_fixed(train_losses, val_losses, train_accs, val_accs,\n",
    "                               title=\"ResNet-50 Training Curves (Fixed Split)\"):\n",
    "    \"\"\"Plot single-run curves.\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(12,5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", linewidth=2)\n",
    "    plt.plot(epochs, val_losses, label=\"Val Loss\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, train_accs, label=\"Train Acc\", linewidth=2)\n",
    "    plt.plot(epochs, val_accs, label=\"Val Acc\", linewidth=2)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Auto-Plot depending on mode\n",
    "# ============================================================\n",
    "\n",
    "if use_kfold:\n",
    "    # If you tracked fold histories manually inside training loop:\n",
    "    try:\n",
    "        plot_training_curves_kfold(fold_histories, num_folds)\n",
    "    except NameError:\n",
    "        print(\" No 'fold_histories' found.\")\n",
    "else:\n",
    "    try:\n",
    "        plot_training_curves_fixed(\n",
    "            fold_histories[0]['train_losses'],\n",
    "            fold_histories[0]['val_losses'],\n",
    "            fold_histories[0]['train_accs'],\n",
    "            fold_histories[0]['val_accs']\n",
    "        )\n",
    "    except NameError:\n",
    "        print(\" No training history detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135cce3-f0cd-4791-a6dd-0e6c116445dc",
   "metadata": {},
   "source": [
    "##### 10. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd0ba35-5bba-439c-a315-772b220c6447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the best model (from Cell 8)\n",
    "best_model_path = \"best_resnet50_fire.pth\" if not use_kfold else \"best_resnet50_fire_fold1.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in tqdm(test_loader, desc=\"Evaluating\", ncols=90):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "# ---- Metrics ----\n",
    "print(\"\\n✅ Classification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=train_dataset.classes, digits=4))\n",
    "\n",
    "# ---- Confusion Matrix ----\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix on Test Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f5563-d09b-4467-8c42-9d279d950755",
   "metadata": {},
   "source": [
    "##### 11. Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c36f95-c012-41a2-be87-98b1983874a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "# Pick random indices from test dataset\n",
    "indices = random.sample(range(len(test_dataset)), 6)\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "for i, idx in enumerate(indices):\n",
    "    img, label = test_dataset[idx]\n",
    "    with torch.no_grad():\n",
    "        output = model(img.unsqueeze(0).to(device))\n",
    "        _, pred = torch.max(output, 1)\n",
    "    pred_label = train_dataset.classes[pred.item()]\n",
    "    true_label = train_dataset.classes[label]\n",
    "    color = \"green\" if pred_label == true_label else \"red\"\n",
    "\n",
    "    plt.subplot(2,3,i+1)\n",
    "    plt.imshow(np.transpose(img.cpu().numpy(), (1,2,0)))\n",
    "    plt.title(f\"Pred: {pred_label} | True: {true_label}\", color=color, fontsize=10)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Example Predictions on Test Images\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bdaf6-e787-4e9d-a2f9-e1a447e2b474",
   "metadata": {},
   "source": [
    "##### 12. Cross-Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4f935f-a0c7-4d58-a80e-091fa4df70be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to where all external datasets are stored\n",
    "base_path = \"C:/Users/shaerib/OneDrive - Clemson University/Desktop/Dataset\"\n",
    "\n",
    "# Toggle which datasets you want to evaluate (True = test, False = skip)\n",
    "test_datasets = {\n",
    "    \"WildFireCan\":     False,\n",
    "    \"Fire360\":         False,\n",
    "    \"DisasterM3\":      False,\n",
    "    \"MultimodalGas\":   False,\n",
    "    \"FLAME-SD\":        False,\n",
    "    \"FLAME3\":          False,\n",
    "    \"FLAME2\":          False,\n",
    "    \"FLAME1\":          True,\n",
    "    \"DFS\":             False,\n",
    "    \"BA-UAV\":          False,\n",
    "    \"FireDetn\":        False,\n",
    "    \"FireFly\":         False,\n",
    "    \"DeepFire\":        False,\n",
    "    \"Paddle Fire\":     False,\n",
    "    \"DataCluster\":     False,\n",
    "    \"WildfireDet\":     False,\n",
    "    \"D-Fire\":          False,\n",
    "    \"FF-Det\":          False,\n",
    "    \"FireNet\":         False,\n",
    "    \"AIDER\":           False,\n",
    "    \"ForestryImage\":   False,\n",
    "    \"Fire\":            True,\n",
    "    \"CAIR\":            False,\n",
    "    \"FESB MLID\":       False,\n",
    "    \"FiSmo\":           False,\n",
    "    \"Corsican\":        False,\n",
    "    \"FireSense\":       False,\n",
    "    \"BoWFire\":         False,\n",
    "    \"MIVIA\":           False,\n",
    "    \"VisiFire\":        False,\n",
    "    \"FireClips\":       False,\n",
    "    \"FSC22\":           False,\n",
    "    \"FireSound\":       False,\n",
    "    \"IberFire\":        False,\n",
    "    \"Algerian\":        False,\n",
    "    \"SmokeDet\":        False,\n",
    "    \"EarlyFire\":       False,\n",
    "}\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_on_dataset(model, dataset_path, device=\"cuda\"):\n",
    "    test_dir = os.path.join(dataset_path, \"test\")\n",
    "    if not os.path.exists(test_dir):\n",
    "        print(f\"Skipping {dataset_path} — no 'test' folder found.\")\n",
    "        return None\n",
    "\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=val_test_transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Accuracy & Report\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    report = classification_report(y_true, y_pred, target_names=test_dataset.classes, digits=3)\n",
    "    return acc, report\n",
    "\n",
    "\n",
    "# Load the best trained model (from your main dataset)\n",
    "model_path = \"best_resnet50_fire.pth\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = models.resnet50(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 2)   # binary fire / no_fire\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate sequentially\n",
    "results = {}\n",
    "\n",
    "print(\"\\n Starting Cross-Dataset Evaluation ...\\n\")\n",
    "for name, active in test_datasets.items():\n",
    "    if not active:\n",
    "        continue\n",
    "\n",
    "    dataset_dir = os.path.join(base_path, name)\n",
    "    if not os.path.exists(dataset_dir):\n",
    "        print(f\"Dataset not found: {dataset_dir}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n=====  Evaluating on {name} =====\")\n",
    "    acc, report = evaluate_on_dataset(model, dataset_dir, device=device)\n",
    "    if acc is not None:\n",
    "        print(f\"\\n Accuracy on {name}: {acc*100:.2f}%\")\n",
    "        print(report)\n",
    "        results[name] = acc\n",
    "    else:\n",
    "        print(f\" Evaluation skipped for {name}.\")\n",
    "\n",
    "print(\"\\n📊 Summary of Results:\")\n",
    "for name, acc in results.items():\n",
    "    print(f\"  {name:<20}: {acc*100:.2f}%\")\n",
    "\n",
    "if results:\n",
    "    avg_acc = np.mean(list(results.values()))\n",
    "    print(f\"\\n Average Accuracy across tested datasets: {avg_acc*100:.2f}%\")\n",
    "else:\n",
    "    print(\" No datasets evaluated — set at least one to True in test_datasets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede76627-5a2a-4701-835c-df8959c3f212",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
