{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09720fc2-5d29-438e-8799-c06c21033e1f",
   "metadata": {},
   "source": [
    "# Segmentation ‚Äî DeepLab-V3\n",
    "**Author:** Sayed Pedram Haeri Boroujeni  \n",
    "**Position:** PhD Student, Clemson University  \n",
    "**Affiliation:** Department of Computer Science  \n",
    "**Email:** shaerib@g.clemson.edu  \n",
    "**Date Created:** October 10, 2025  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f649a-a69d-43e3-9aa0-9537063b5a8d",
   "metadata": {},
   "source": [
    "##### 1. Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2501c692-3069-455b-b3b8-e9dff9873a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, random, time, math\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.transforms import functional as TF "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f37870-4b7b-467d-85c0-776bc6761585",
   "metadata": {},
   "source": [
    "##### 2. Checking GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f74139-a333-411b-bc84-d741371b4f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA Available: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"CUDA is not available on this system.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae7d4a-68a3-4173-8746-e2e63bb0970b",
   "metadata": {},
   "source": [
    "##### 3. Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd5c8b-0d43-41a1-87fb-93a997dde6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e7140-d5d8-4527-9865-6ec577569a7e",
   "metadata": {},
   "source": [
    "##### 4. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00abb404-0e46-43a6-b188-59d86148e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir = \"ADD YOUR PATH HERE\"  \n",
    "data_dir = \"C:/Users/shaerib/OneDrive - Clemson University/Desktop/Dataset/FESB MLID\"  \n",
    "\n",
    "\n",
    "use_kfold = True\n",
    "num_folds = 5\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "img_size = 513"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3f7630-add8-452f-866f-12c264786ed7",
   "metadata": {},
   "source": [
    "##### 5. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298668ea-3ca2-46af-80bd-110ec7c7c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_segmentation_dataset(data_dir, img_size=513):\n",
    "    \"\"\"\n",
    "    Prepare dataset for segmentation:\n",
    "    1. Detect paired image and ground truth (GT) mask files.\n",
    "    2. Ensure train/test split (if 'val' exists, keep it).\n",
    "    3. Resize and rename all images & masks sequentially (1, 2, 3, ...).\n",
    "    4. Save processed images in 'src/' and masks in 'gt/' folders under each split.\n",
    "    5. Print total sample counts.\n",
    "    \"\"\"\n",
    "\n",
    "    from tqdm import tqdm\n",
    "    from PIL import Image\n",
    "    from pathlib import Path\n",
    "\n",
    "    data_dir = Path(data_dir)\n",
    "    exts_img = [\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\", \"*.tiff\", \"*.webp\"]\n",
    "\n",
    "    # --- Helper to find all images in folder ---\n",
    "    def all_images_in(folder):\n",
    "        imgs = []\n",
    "        for ext in exts_img:\n",
    "            imgs.extend(Path(folder).rglob(ext))\n",
    "        return imgs\n",
    "\n",
    "    # --- Detect main folders ---\n",
    "    train_dir = data_dir / \"train\"\n",
    "    val_dir = data_dir / \"val\"\n",
    "    test_dir = data_dir / \"test\"\n",
    "\n",
    "    # --- Check existence ---\n",
    "    splits = []\n",
    "    if train_dir.exists(): splits.append(train_dir)\n",
    "    if val_dir.exists(): splits.append(val_dir)\n",
    "    if test_dir.exists(): splits.append(test_dir)\n",
    "    if not splits:\n",
    "        raise RuntimeError(\"No train/test folders found. Expected at least 'train/' and 'test/'.\")\n",
    "\n",
    "    # --- Iterate over splits ---\n",
    "    for split_dir in splits:\n",
    "        print(f\"\\n‚öôÔ∏è Processing split: {split_dir.name}\")\n",
    "\n",
    "        # Detect subfolders\n",
    "        img_dir = split_dir / \"src\"\n",
    "        gt_dir  = split_dir / \"gt\"\n",
    "\n",
    "        if not img_dir.exists() or not gt_dir.exists():\n",
    "            raise RuntimeError(f\"Expected 'images/' and 'gt/' folders inside {split_dir}\")\n",
    "\n",
    "        img_files = sorted(all_images_in(img_dir))\n",
    "        gt_files  = sorted(all_images_in(gt_dir))\n",
    "        if len(img_files) == 0 or len(gt_files) == 0:\n",
    "            print(f\"‚ö†Ô∏è No images or GT masks found in {split_dir}\")\n",
    "            continue\n",
    "\n",
    "        # --- Check pairs and match ---\n",
    "        num_pairs = min(len(img_files), len(gt_files))\n",
    "        print(f\"  Found {num_pairs} image‚Äìmask pairs.\")\n",
    "\n",
    "        # --- Create output folders ---\n",
    "        out_img_dir = split_dir / \"src_generated\"\n",
    "        out_gt_dir  = split_dir / \"gt_generated\"\n",
    "        out_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "        out_gt_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # --- Process pairs ---\n",
    "        for i in tqdm(range(num_pairs), desc=f\"{split_dir.name}\", ncols=90):\n",
    "            try:\n",
    "                img = Image.open(img_files[i]).convert(\"RGB\")\n",
    "                gt  = Image.open(gt_files[i]).convert(\"L\")\n",
    "\n",
    "                img = img.resize((img_size, img_size))\n",
    "                gt  = gt.resize((img_size, img_size), resample=Image.NEAREST)\n",
    "\n",
    "                # filenames start from 1\n",
    "                idx = i + 1\n",
    "\n",
    "                img.save(out_img_dir / f\"{idx}.jpg\", \"JPEG\", quality=95)\n",
    "                gt.save(out_gt_dir / f\"{idx}.png\", \"PNG\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error processing {img_files[i]}: {e}\")\n",
    "\n",
    "        print(f\"‚úÖ {split_dir.name}: Saved {num_pairs} paired samples to '{out_img_dir.name}' and '{out_gt_dir.name}'.\")\n",
    "\n",
    "    print(\"\\nüéØ Segmentation dataset preparation completed successfully!\")\n",
    "\n",
    "#Test\n",
    "prepare_segmentation_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d48e228-563c-4912-88f8-2f5f55826f46",
   "metadata": {},
   "source": [
    "##### 6. Data Loading & Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eaca8a3-0004-4b79-93f3-2e3e589d4f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image normalization (ImageNet values)\n",
    "mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "img_size = 513  # DeepLab-V3 default input size\n",
    "\n",
    "# --- Define transforms ---\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size), interpolation=Image.NEAREST),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Custom Dataset for paired src and gt_generated\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.images = sorted(os.listdir(image_dir))\n",
    "        self.masks = sorted(os.listdir(mask_dir))\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.images[idx])\n",
    "        mask_path = os.path.join(self.mask_dir, self.masks[idx])\n",
    "\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            mask = self.target_transform(mask)\n",
    "\n",
    "        mask = (mask > 0).long()  # binary mask\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "# Initialize Datasets\n",
    "train_dataset = SegmentationDataset(\n",
    "    os.path.join(data_dir, \"train\", \"src_generated\"),\n",
    "    os.path.join(data_dir, \"train\", \"gt_generated\"),\n",
    "    transform=train_transform,\n",
    "    target_transform=mask_transform\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    os.path.join(data_dir, \"val\", \"src_generated\"),\n",
    "    os.path.join(data_dir, \"val\", \"gt_generated\"),\n",
    "    transform=val_transform,\n",
    "    target_transform=mask_transform\n",
    ")\n",
    "\n",
    "test_dataset = SegmentationDataset(\n",
    "    os.path.join(data_dir, \"test\", \"src_generated\"),\n",
    "    os.path.join(data_dir, \"test\", \"gt_generated\"),\n",
    "    transform=test_transform,\n",
    "    target_transform=mask_transform\n",
    ")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06284569-ddb6-47c3-b7d8-ffdbd32a3e57",
   "metadata": {},
   "source": [
    "##### 7. DeepLab-V3 Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e37f6fe-b0df-4cee-8803-d22896fb554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "model = deeplabv3_resnet50(weights=weights)\n",
    "model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)  # two classes: background & fire\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(momentum, 0.999))\n",
    "\n",
    "print(\"DeepLab-V3 initialized and ready for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4957df-53d4-4b03-a34c-40468c585ccb",
   "metadata": {},
   "source": [
    "##### 8. DeepLab-V3 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd417ec-69f2-47f6-9bbd-37b2a5cf50c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_segmentation(model, criterion, optimizer, train_loader, val_loader,\n",
    "                       num_epochs, device='cuda', fold=None):\n",
    "    \"\"\"Training loop with validation/test, best model saving, and IoU tracking.\"\"\"\n",
    "    train_losses, val_losses, val_ious = [], [], []\n",
    "    best_iou = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # ---- Training ----\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Fold {fold or 0} | Epoch {epoch}/{num_epochs}\", ncols=90):\n",
    "            imgs, masks = imgs.to(device), masks.squeeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)[\"out\"]\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # ---- Validation/Test ----\n",
    "        model.eval()\n",
    "        val_loss, iou_scores = 0.0, []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs, masks = imgs.to(device), masks.squeeze(1).to(device)\n",
    "                outputs = model(imgs)[\"out\"]\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item() * imgs.size(0)\n",
    "\n",
    "                preds = torch.argmax(outputs, 1)\n",
    "                intersection = (preds & masks).float().sum((1,2))\n",
    "                union = (preds | masks).float().sum((1,2))\n",
    "                iou_scores.extend((intersection / (union + 1e-6)).cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        mean_iou = np.mean(iou_scores)\n",
    "        val_losses.append(val_loss)\n",
    "        val_ious.append(mean_iou)\n",
    "\n",
    "        print(f\"[Fold {fold or 0}] Epoch {epoch:02d}/{num_epochs} | \"\n",
    "              f\"Train Loss {epoch_loss:.4f} | Val Loss {val_loss:.4f} | mIoU {mean_iou:.4f}\")\n",
    "\n",
    "        # ---- Save best model per fold ----\n",
    "        if mean_iou > best_iou:\n",
    "            best_iou = mean_iou\n",
    "            model_name = f\"best_deeplabv3_segmentation_fold{fold}.pth\" if fold else \"best_deeplabv3_segmentation.pth\"\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "\n",
    "    print(f\"\\n‚úÖ Training completed for fold {fold or 0}. Best mIoU = {best_iou:.4f}\")\n",
    "    return train_losses, val_losses, val_ious, best_iou\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  K-Fold Cross-Validation or Fixed Split (Safe Version)\n",
    "# ============================================================\n",
    "\n",
    "if use_kfold:\n",
    "    print(f\"üîÅ Running {num_folds}-Fold Cross-Validation ‚Ä¶\\n\")\n",
    "\n",
    "    # Prepare full dataset (train + pseudo-labels for stratification)\n",
    "    full_dataset = SegmentationDataset(\n",
    "        os.path.join(data_dir, \"train\", \"src_generated\"),\n",
    "        os.path.join(data_dir, \"train\", \"gt_generated\"),\n",
    "        transform=train_transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "\n",
    "    # Pseudo-labels (mean mask intensity > 0 ‚áí class 1)\n",
    "    labels_for_split = []\n",
    "    for _, mask in full_dataset:\n",
    "        labels_for_split.append(int(mask.float().mean().item() > 0.01))\n",
    "    labels_for_split = np.array(labels_for_split)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    fold_results = []\n",
    "    fold_histories = {}\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels_for_split)), labels_for_split)):\n",
    "        print(f\"\\n===== Fold {fold+1}/{num_folds} =====\")\n",
    "\n",
    "        train_subset = Subset(full_dataset, train_idx)\n",
    "        val_subset   = Subset(full_dataset, val_idx)\n",
    "\n",
    "        # SAFE DataLoaders (no multiprocessing)\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True,\n",
    "                                  num_workers=0, pin_memory=True)\n",
    "        val_loader   = DataLoader(val_subset, batch_size=batch_size, shuffle=False,\n",
    "                                  num_workers=0, pin_memory=True)\n",
    "\n",
    "        # Re-initialize model per fold\n",
    "        model = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1)\n",
    "        model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)\n",
    "        if hasattr(model, \"aux_classifier\"):\n",
    "            model.aux_classifier = None\n",
    "        model = model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(momentum, 0.999))\n",
    "\n",
    "        tr_loss, vl_loss, vl_iou, best_iou = train_segmentation(\n",
    "            model, criterion, optimizer, train_loader, val_loader,\n",
    "            num_epochs=num_epochs, device=device, fold=fold+1\n",
    "        )\n",
    "\n",
    "        fold_results.append(best_iou)\n",
    "        fold_histories[fold+1] = {\n",
    "            \"train_losses\": tr_loss,\n",
    "            \"val_losses\": vl_loss,\n",
    "            \"val_ious\": vl_iou,\n",
    "            \"best_iou\": best_iou\n",
    "        }\n",
    "\n",
    "    print(\"\\n‚úÖ Cross-Validation Complete.\")\n",
    "    print(f\"Average mIoU = {np.mean(fold_results):.4f} ¬± {np.std(fold_results):.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚û°Ô∏è Using fixed train/val/test split.\\n\")\n",
    "\n",
    "    # SAFE DataLoaders for fixed split\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=0, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=0, pin_memory=True)\n",
    "\n",
    "    # ‚úÖ Use val_loader for validation (keep test for final eval)\n",
    "    tr_loss, vl_loss, vl_iou, best_iou = train_segmentation(\n",
    "        model, criterion, optimizer, train_loader, val_loader,\n",
    "        num_epochs=num_epochs, device=device\n",
    "    )\n",
    "\n",
    "    fold_histories = {\n",
    "        0: {\n",
    "            \"train_losses\": tr_loss,\n",
    "            \"val_losses\": vl_loss,\n",
    "            \"val_ious\": vl_iou,\n",
    "            \"best_iou\": best_iou\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Fixed-split training complete. Best Val mIoU = {best_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9c962-08f1-4cdf-9b2d-9f79a4f58d50",
   "metadata": {},
   "source": [
    "##### 9. Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06c6e22-4a66-425c-952e-db4703ffa1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_segmentation_curves(fold_histories, use_kfold=False):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # ============================================================\n",
    "    # Training vs Validation Loss\n",
    "    # ============================================================\n",
    "    plt.subplot(1, 2, 1)\n",
    "    color_cycle = plt.cm.tab10.colors  # consistent color palette\n",
    "    for i, (fold, hist) in enumerate(fold_histories.items()):\n",
    "        color = color_cycle[i % len(color_cycle)]\n",
    "        label_prefix = f\"Fold {fold}\" if use_kfold else \"Run\"\n",
    "        plt.plot(hist[\"train_losses\"], label=f\"{label_prefix} Train\", color=color, linestyle='-')\n",
    "        plt.plot(hist[\"val_losses\"],   label=f\"{label_prefix} Val\",   color='Orange', linestyle='--')\n",
    "\n",
    "    plt.title(\"Training vs Validation Loss\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend(fontsize=8)\n",
    "\n",
    "    # ============================================================\n",
    "    # Validation mIoU Curves\n",
    "    # ============================================================\n",
    "    plt.subplot(1, 2, 2)\n",
    "    all_val_ious = []\n",
    "\n",
    "    for i, (fold, hist) in enumerate(fold_histories.items()):\n",
    "        color = color_cycle[i % len(color_cycle)]\n",
    "        label_prefix = f\"Fold {fold}\" if use_kfold else \"Run\"\n",
    "        plt.plot(hist[\"val_ious\"], label=f\"{label_prefix} mIoU\", linewidth=2, alpha=0.9, color=color)\n",
    "        all_val_ious.append(hist[\"val_ious\"])\n",
    "\n",
    "    # Plot average mIoU curve for K-Fold\n",
    "    if use_kfold and len(all_val_ious) > 1:\n",
    "        min_len = min(len(v) for v in all_val_ious)\n",
    "        mean_iou = np.mean([v[:min_len] for v in all_val_ious], axis=0)\n",
    "        plt.plot(mean_iou, color='black', linewidth=3, label=\"Average mIoU\", linestyle='-.')\n",
    "\n",
    "    plt.title(\"Validation mIoU per Epoch\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean IoU\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "    plt.legend(fontsize=8)\n",
    "\n",
    "    plt.suptitle(\"DeepLab-V3 Training Progress\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Run Plot Function\n",
    "# ============================================================\n",
    "if \"fold_histories\" in locals() and len(fold_histories) > 0:\n",
    "    plot_segmentation_curves(fold_histories, use_kfold=use_kfold)\n",
    "else:\n",
    "    print(\" No training history found. Please run Cell 8 first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135cce3-f0cd-4791-a6dd-0e6c116445dc",
   "metadata": {},
   "source": [
    "##### 10. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae526d7-7c72-4f7f-9f6d-99c35ca0c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_segmentation_model(model, test_loader, device='cuda'):\n",
    "    \"\"\"Evaluate DeepLabV3 on the test set and compute IoU, F1, Accuracy, and Specificity.\"\"\"\n",
    "    model.eval()\n",
    "    ious, f1s, accuracies, specificities = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(test_loader, desc=\"Evaluating on Test Set\", ncols=90):\n",
    "            imgs, masks = imgs.to(device), masks.squeeze(1).to(device)\n",
    "            outputs = model(imgs)[\"out\"]\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            # Compute metrics per batch\n",
    "            intersection = (preds & masks).float().sum((1, 2))\n",
    "            union = (preds | masks).float().sum((1, 2))\n",
    "            iou = (intersection / (union + 1e-6)).cpu().numpy()\n",
    "\n",
    "            tp = ((preds == 1) & (masks == 1)).sum().item()\n",
    "            tn = ((preds == 0) & (masks == 0)).sum().item()\n",
    "            fp = ((preds == 1) & (masks == 0)).sum().item()\n",
    "            fn = ((preds == 0) & (masks == 1)).sum().item()\n",
    "\n",
    "            f1 = (2 * tp) / (2 * tp + fp + fn + 1e-6)\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "            spec = tn / (tn + fp + 1e-6)\n",
    "\n",
    "            ious.extend(iou)\n",
    "            f1s.append(f1)\n",
    "            accuracies.append(acc)\n",
    "            specificities.append(spec)\n",
    "\n",
    "    # Aggregate results\n",
    "    mean_iou = np.mean(ious)\n",
    "    mean_f1 = np.mean(f1s)\n",
    "    mean_acc = np.mean(accuracies)\n",
    "    mean_spec = np.mean(specificities)\n",
    "\n",
    "    print(f\"\\n‚úÖ Test Evaluation Complete:\")\n",
    "    print(f\"  Mean IoU:         {mean_iou:.4f}\")\n",
    "    print(f\"  Mean F1-Score:    {mean_f1:.4f}\")\n",
    "    print(f\"  Mean Accuracy:    {mean_acc:.4f}\")\n",
    "    print(f\"  Mean Specificity: {mean_spec:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"IoU\": mean_iou,\n",
    "        \"F1\": mean_f1,\n",
    "        \"Accuracy\": mean_acc,\n",
    "        \"Specificity\": mean_spec\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load Best Model(s) and Evaluate\n",
    "# ============================================================\n",
    "\n",
    "if use_kfold:\n",
    "    print(\"\\nüîç Evaluating best model from each fold on the test set...\\n\")\n",
    "    test_results = {}\n",
    "\n",
    "    for fold in range(1, num_folds + 1):\n",
    "        model_path = f\"best_deeplabv3_segmentation_fold{fold}.pth\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(f\"‚ö†Ô∏è Model checkpoint not found: {model_path}\")\n",
    "            continue\n",
    "\n",
    "        # Reload model for this fold\n",
    "        model = deeplabv3_resnet50(weights=None)\n",
    "        model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)\n",
    "        if hasattr(model, \"aux_classifier\"):\n",
    "            model.aux_classifier = None  # avoid key mismatch errors\n",
    "\n",
    "        # Load weights safely\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "        if unexpected:\n",
    "            print(f\"   Ignored keys during loading: {unexpected}\")\n",
    "\n",
    "        model = model.to(device)\n",
    "\n",
    "        print(f\"\\n===== Evaluating Fold {fold} =====\")\n",
    "        metrics = evaluate_segmentation_model(model, test_loader, device=device)\n",
    "        test_results[fold] = metrics\n",
    "\n",
    "    # Average metrics across folds\n",
    "    if len(test_results) > 0:\n",
    "        metric_names = list(test_results[list(test_results.keys())[0]].keys())\n",
    "        avg_metrics = {m: np.mean([r[m] for r in test_results.values()]) for m in metric_names}\n",
    "\n",
    "        print(\"\\nüìä Average Test Metrics Across Folds:\")\n",
    "        for k, v in avg_metrics.items():\n",
    "            print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüîç Evaluating single best model on test set...\\n\")\n",
    "\n",
    "    model_path = \"best_deeplabv3_segmentation.pth\"\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(\"‚ùå Best model checkpoint not found. Run training first.\")\n",
    "\n",
    "    # Load best model\n",
    "    model = deeplabv3_resnet50(weights=None)\n",
    "    model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)\n",
    "    if hasattr(model, \"aux_classifier\"):\n",
    "        model.aux_classifier = None\n",
    "\n",
    "    # Load weights safely\n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    if unexpected:\n",
    "        print(f\"Ignored keys during loading: {unexpected}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Evaluate on held-out test set \n",
    "    test_metrics = evaluate_segmentation_model(model, test_loader, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987f5563-d09b-4467-8c42-9d279d950755",
   "metadata": {},
   "source": [
    "##### 11. Example Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c36f95-c012-41a2-be87-98b1983874a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ImageNet normalization (same as used in training) ---\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std  = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "def denormalize(tensor):\n",
    "    \"\"\"Undo normalization for visualization.\"\"\"\n",
    "    return torch.clamp(tensor * std + mean, 0, 1)\n",
    "\n",
    "def visualize_segmentation_predictions(model, dataset, device='cuda', num_samples=5):\n",
    "    \"\"\"Visualize segmentation predictions using true-color images.\"\"\"\n",
    "    model.eval()\n",
    "    indices = random.sample(range(len(dataset)), min(num_samples, len(dataset)))\n",
    "\n",
    "    plt.figure(figsize=(15, num_samples * 3))\n",
    "    for i, idx in enumerate(indices):\n",
    "        img, gt = dataset[idx]\n",
    "        img_tensor = img.unsqueeze(0).to(device)\n",
    "        gt_tensor = gt.squeeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(img_tensor)[\"out\"]\n",
    "            pred = torch.argmax(output, dim=1).squeeze(0).cpu()\n",
    "\n",
    "        # --- Denormalize image for true color display\n",
    "        img_denorm = denormalize(img_tensor.squeeze(0).cpu())\n",
    "        img_disp = TF.to_pil_image(img_denorm)\n",
    "\n",
    "        # Convert masks to grayscale images\n",
    "        gt_disp = Image.fromarray((gt_tensor.numpy() * 255).astype(np.uint8))\n",
    "        pred_disp = Image.fromarray((pred.numpy() * 255).astype(np.uint8))\n",
    "\n",
    "        # Plot triple: Input / Ground Truth / Prediction\n",
    "        plt.subplot(num_samples, 3, 3*i + 1)\n",
    "        plt.imshow(img_disp)\n",
    "        plt.title(\"Original Image\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(num_samples, 3, 3*i + 2)\n",
    "        plt.imshow(gt_disp, cmap=\"gray\")\n",
    "        plt.title(\"Ground Truth Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(num_samples, 3, 3*i + 3)\n",
    "        plt.imshow(pred_disp, cmap=\"gray\")\n",
    "        plt.title(\"Predicted Mask\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load Best Model and Visualize\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n Generating Example Predictions \\n\")\n",
    "\n",
    "if use_kfold:\n",
    "    best_fold = max(fold_histories, key=lambda f: fold_histories[f][\"best_iou\"])\n",
    "    model_path = f\"best_deeplabv3_segmentation_fold{best_fold}.pth\"\n",
    "    print(f\"Using best fold model: Fold {best_fold}\")\n",
    "else:\n",
    "    model_path = \"best_deeplabv3_segmentation.pth\"\n",
    "\n",
    "# Load model safely\n",
    "model = deeplabv3_resnet50(weights=None)\n",
    "model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "if unexpected:\n",
    "    print(f\" Ignored keys during loading: {unexpected}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Visualize predictions\n",
    "visualize_segmentation_predictions(model, test_dataset, device=device, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20bdaf6-e787-4e9d-a2f9-e1a447e2b474",
   "metadata": {},
   "source": [
    "##### 12. Cross-Dataset Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede76627-5a2a-4701-835c-df8959c3f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"C:/Users/shaerib/OneDrive - Clemson University/Desktop/Dataset\"\n",
    "\n",
    "# Toggle which datasets you want to evaluate \n",
    "test_datasets = {\n",
    "    \"FLAME1\":    False,\n",
    "    \"BA-UAV\":    False,\n",
    "    \"BoWFire\":   False,\n",
    "    \"FESB MLID\": True,\n",
    "}\n",
    "\n",
    "# Evaluation \n",
    "def evaluate_segmentation_on_dataset(model, dataset_path, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluate DeepLab-V3 segmentation model on an external dataset.\n",
    "    Expects folder structure:\n",
    "        dataset_path/\n",
    "            test/\n",
    "                src/ (images)\n",
    "                gt/  (ground truth masks)\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    test_dir = dataset_path / \"test\"\n",
    "\n",
    "    if not test_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è Skipping {dataset_path.name} ‚Äî no 'test/' folder found.\")\n",
    "        return None\n",
    "\n",
    "    img_dir = test_dir / \"src_generated\"\n",
    "    gt_dir  = test_dir / \"gt_generated\"\n",
    "    if not img_dir.exists() or not gt_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è Skipping {dataset_path.name} ‚Äî missing 'src_generated' or 'gt_generated' folder.\")\n",
    "        return None\n",
    "\n",
    "    # Create dataset + loader\n",
    "    test_dataset = SegmentationDataset(\n",
    "        image_dir=str(img_dir),\n",
    "        mask_dir=str(gt_dir),\n",
    "        transform=test_transform,\n",
    "        target_transform=mask_transform\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    ious, f1s, accuracies, specificities = [], [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, masks in tqdm(test_loader, desc=f\"Evaluating {dataset_path.name}\", ncols=90):\n",
    "            imgs, masks = imgs.to(device), masks.squeeze(1).to(device)\n",
    "            outputs = model(imgs)[\"out\"]\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "            # --- Compute metrics per batch ---\n",
    "            intersection = (preds & masks).float().sum((1, 2))\n",
    "            union = (preds | masks).float().sum((1, 2))\n",
    "            iou = (intersection / (union + 1e-6)).cpu().numpy()\n",
    "\n",
    "            tp = ((preds == 1) & (masks == 1)).sum().item()\n",
    "            tn = ((preds == 0) & (masks == 0)).sum().item()\n",
    "            fp = ((preds == 1) & (masks == 0)).sum().item()\n",
    "            fn = ((preds == 0) & (masks == 1)).sum().item()\n",
    "\n",
    "            f1 = (2 * tp) / (2 * tp + fp + fn + 1e-6)\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
    "            spec = tn / (tn + fp + 1e-6)\n",
    "\n",
    "            ious.extend(iou)\n",
    "            f1s.append(f1)\n",
    "            accuracies.append(acc)\n",
    "            specificities.append(spec)\n",
    "\n",
    "    # Aggregate metrics\n",
    "    return {\n",
    "        \"IoU\": np.mean(ious),\n",
    "        \"F1\": np.mean(f1s),\n",
    "        \"Accuracy\": np.mean(accuracies),\n",
    "        \"Specificity\": np.mean(specificities),\n",
    "    }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Load the best trained segmentation model\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüöÄ Starting Cross-Dataset Evaluation ...\\n\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if use_kfold:\n",
    "    best_fold = max(fold_histories, key=lambda f: fold_histories[f][\"best_iou\"])\n",
    "    model_path = f\"best_deeplabv3_segmentation_fold{best_fold}.pth\"\n",
    "    print(f\"üß† Using best fold model: Fold {best_fold}\")\n",
    "else:\n",
    "    model_path = \"best_deeplabv3_segmentation.pth\"\n",
    "\n",
    "# --- Load DeepLab-V3 model safely ---\n",
    "model = deeplabv3_resnet50(weights=None)\n",
    "model.classifier[4] = nn.Conv2d(256, 2, kernel_size=1)\n",
    "if hasattr(model, \"aux_classifier\"):\n",
    "    model.aux_classifier = None\n",
    "\n",
    "state_dict = torch.load(model_path, map_location=device)\n",
    "missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "if unexpected:\n",
    "    print(f\"‚ö†Ô∏è Ignored keys during loading: {unexpected}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# ============================================================\n",
    "# Evaluate Sequentially Across Datasets\n",
    "# ============================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, active in test_datasets.items():\n",
    "    if not active:\n",
    "        continue\n",
    "\n",
    "    dataset_dir = Path(base_path) / name\n",
    "    if not dataset_dir.exists():\n",
    "        print(f\"‚ùå Dataset not found: {dataset_dir}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n===== Evaluating on {name} =====\")\n",
    "    metrics = evaluate_segmentation_on_dataset(model, dataset_dir, device=device)\n",
    "    if metrics is not None:\n",
    "        print(f\"\\nüìä Results on {name}:\")\n",
    "        for k, v in metrics.items():\n",
    "            print(f\"  {k:<12}: {v:.4f}\")\n",
    "        results[name] = metrics\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Evaluation skipped for {name}.\")\n",
    "\n",
    "# ============================================================\n",
    "# Summary of Results\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\nüìà Summary of Cross-Dataset Results:\")\n",
    "for name, m in results.items():\n",
    "    print(f\"  {name:<20} | IoU: {m['IoU']:.4f} | F1: {m['F1']:.4f} | Acc: {m['Accuracy']:.4f} | Spec: {m['Specificity']:.4f}\")\n",
    "\n",
    "if results:\n",
    "    avg_metrics = {k: np.mean([v[k] for v in results.values()]) for k in [\"IoU\", \"F1\", \"Accuracy\", \"Specificity\"]}\n",
    "    print(\"\\n‚≠ê Average Across Tested Datasets:\")\n",
    "    for k, v in avg_metrics.items():\n",
    "        print(f\"  {k:<12}: {v:.4f}\")\n",
    "else:\n",
    "    print(\"‚ùå No datasets evaluated ‚Äî enable at least one in test_datasets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f60676-7de2-41d6-860f-3aa3211d4e10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
