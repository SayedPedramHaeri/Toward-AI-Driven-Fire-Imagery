## ğŸ” Overview
This module benchmarks **CLIP (Contrastive Languageâ€“Image Pretraining)** for wildfire classification through **imageâ€“text similarity**.  
The model is used to evaluate zero-shot and few-shot performance across multiple datasets using descriptive textual prompts (e.g., â€œa photo of wildfireâ€ vs. â€œa photo of forest without fireâ€).  
The official **OpenAI CLIP** implementation and pretrained **ViT-B/32** backbone are employed to measure the capability of visionâ€“language models in cross-modal wildfire understanding.
