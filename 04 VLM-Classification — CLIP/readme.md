## ğŸ” Overview
This module benchmarks **CLIP (Contrastive Languageâ€“Image Pretraining)** for wildfire classification through **imageâ€“text similarity**.  
The model is used to evaluate zero-shot and few-shot performance across multiple datasets using descriptive textual prompts (e.g., â€œa photo of fireâ€ vs. â€œa photo of no fireâ€). The official **OpenAI CLIP** implementation and pretrained **ViT-B/32** backbone are employed to measure the capability of visionâ€“language models in cross-modal wildfire understanding.

---

## âš™ï¸ Setup & Requirements
1. **Install dependencies**
   ```bash
   pip install git+https://github.com/openai/CLIP.git
   pip install torch torchvision torchaudio scikit-learn tqdm seaborn pillow matplotlib
   
git clone https://github.com/<your_repo_name>.git

cd "<your_repo_name>/01 Classification -- CLIP"


## ğŸ§© Data Organization and Preprocessing
Before running the testing scripts, please organize your dataset as follows:

```plaintext
Dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ fire/
â”‚   â””â”€â”€ no_fire/
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ fire/
â”‚   â””â”€â”€ no_fire/
â””â”€â”€ val/
    â”œâ”€â”€ fire/
    â””â”€â”€ no_fire/
```

What This Does:\
1ï¸âƒ£ Verifies that the folder structure exists.\
2ï¸âƒ£ The code detects them automatically and skips re-splitting.\
3ï¸âƒ£ Resizes all files to 224Ã—224 pixels.\
4ï¸âƒ£ Renames them as 1.jpg â†’ 1.png, 2.jpg â†’ 2.png, etc.\
5ï¸âƒ£ Saves processed outputs into the following structure:\

---

## ğŸ§ª Automated Testing on Multiple Datasets

After training, you can automatically evaluate the saved model on the **test subset** of any dataset listed in the repository.  
Simply toggle which datasets to include by setting `True` or `False` in the configuration dictionary, for example:

```python
test_datasets = {
    "WildFireCan": True,
    "Fire360": True,
    "DeepFire": False,
    "FLAME3": True
}

