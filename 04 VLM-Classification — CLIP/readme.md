## 🔍 Overview
This module benchmarks **CLIP (Contrastive Language–Image Pretraining)** for wildfire classification through **image–text similarity**.  
The model is used to evaluate zero-shot and few-shot performance across multiple datasets using descriptive textual prompts (e.g., “a photo of wildfire” vs. “a photo of forest without fire”).  
The official **OpenAI CLIP** implementation and pretrained **ViT-B/32** backbone are employed to measure the capability of vision–language models in cross-modal wildfire understanding.
